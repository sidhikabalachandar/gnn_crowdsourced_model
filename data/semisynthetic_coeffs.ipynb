{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac0f4d50-4f35-41ef-82aa-5791cb46f691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff747456-5d73-4fe7-967c-e293eeeb0e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths to files\n",
    "full_data_file = '/share/garg/311_data/sb2377/clean_codebase/full.h5'\n",
    "base_data_file = '/share/garg/311_data/sb2377/clean_codebase/three_year_base.csv'\n",
    "\n",
    "# define covariates\n",
    "normalized_covariates = ['normalized_log_population_density',\n",
    "                         'normalized_log_income_median',\n",
    "                         'normalized_education_bachelors_pct',\n",
    "                         'normalized_race_white_nh_pct',\n",
    "                         'normalized_age_median',\n",
    "                         'normalized_households_renteroccupied_pct',\n",
    "                         'normalized_rating']\n",
    "thetas = ['theta.{}'.format(i) for i in range(1, len(normalized_covariates))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ad7b51e-5a25-4472-9112-b5ec7759be74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load files\n",
    "data_df = pd.read_hdf(full_data_file, 'df')\n",
    "base_df = pd.read_csv(base_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "970c80ef-d5f3-46c5-8364-1b838fd48902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# formula to create synthetic ratings\n",
    "# r_{ikt} = (1 / alpha_k) * (logit(E_t[T_{ikt}]) - theta_k X_i)\n",
    "\n",
    "# in this notebook we will generate the synthetic coefficients theta_k and alpha_k (except the intercept)\n",
    "# To reflect real-world reporting parameters, theta_k is drawn from N(mu, 0.1)\n",
    "# mu = average reporting coefficients predicted by a logistic regression model run on the real inspection rating data.\n",
    "\n",
    "# alpha_k is set to 1 so that the generated ratings have a standard deviation of 1\n",
    "\n",
    "# the intercept (theta_k[0]) is set such that the generated ratings are mean 0 (see semisynthetic_intercept.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a31c078-0de1-4406-a95c-896e17518700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.028475\n",
      "         Iterations 10\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.158972\n",
      "         Iterations 8\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.013385\n",
      "         Iterations 12\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.045373\n",
      "         Iterations 9\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.094803\n",
      "         Iterations 8\n"
     ]
    }
   ],
   "source": [
    "# get mu = average coefficients\n",
    "# get data with observed ratings\n",
    "observed_df = data_df[data_df['real_rating_observed'] == 1]\n",
    "\n",
    "# get type indices for types with observed ratings\n",
    "type_df = base_df[['typeagency', 'type_idxs']].drop_duplicates()\n",
    "street_idx = type_df[type_df['typeagency'] == 'StreetConditionDOT']['type_idxs'].iloc[0]\n",
    "park_idx = type_df[type_df['typeagency'] == 'MaintenanceorFacilityDPR']['type_idxs'].iloc[0]\n",
    "rodent_idx = type_df[type_df['typeagency'] == 'RodentDOHMH']['type_idxs'].iloc[0]\n",
    "restaurant_idx = type_df[type_df['typeagency'] == 'FoodDOHMH']['type_idxs'].iloc[0]\n",
    "dcwp_idx = type_df[type_df['typeagency'] == 'ConsumerComplaintDCWP']['type_idxs'].iloc[0]\n",
    "\n",
    "# get type specific data with observed ratings\n",
    "street_observed_df = observed_df[observed_df['type_idxs'] == street_idx]\n",
    "park_observed_df = observed_df[observed_df['type_idxs'] == park_idx]\n",
    "rodent_observed_df = observed_df[observed_df['type_idxs'] == rodent_idx]\n",
    "restaurant_observed_df = observed_df[observed_df['type_idxs'] == restaurant_idx]\n",
    "dcwp_observed_df = observed_df[observed_df['type_idxs'] == dcwp_idx]\n",
    "\n",
    "# fit logreg for each type separately\n",
    "formula = 'finegrained_reported ~ '\n",
    "for c in normalized_covariates:\n",
    "    formula += c + ' + '\n",
    "formula = formula[:-3]\n",
    "street_fit = sm.Logit.from_formula(formula, data = street_observed_df).fit()\n",
    "park_fit = sm.Logit.from_formula(formula, data = park_observed_df).fit()\n",
    "rodent_fit = sm.Logit.from_formula(formula, data = rodent_observed_df).fit()\n",
    "restaurant_fit = sm.Logit.from_formula(formula, data = restaurant_observed_df).fit()\n",
    "dcwp_fit = sm.Logit.from_formula(formula, data = dcwp_observed_df).fit()\n",
    "\n",
    "street_coefficients = np.expand_dims(street_fit.params.to_numpy(), axis=1)\n",
    "park_coefficients = np.expand_dims(park_fit.params.to_numpy(), axis=1)\n",
    "rodent_coefficients = np.expand_dims(rodent_fit.params.to_numpy(), axis=1)\n",
    "restaurant_coefficients = np.expand_dims(restaurant_fit.params.to_numpy(), axis=1)\n",
    "dcwp_coefficients = np.expand_dims(dcwp_fit.params.to_numpy(), axis=1)\n",
    "all_coefficients = np.concatenate([street_coefficients, park_coefficients, rodent_coefficients, restaurant_coefficients, dcwp_coefficients], axis=1)\n",
    "\n",
    "# mu = average across types with observed ratings\n",
    "mean_coefficients = all_coefficients.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a5c972-9c82-4f5e-bdba-ef173b789ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type specific theta_k and alpha_k are drawn from N(mu, 0.1)\n",
    "num_sets = 100\n",
    "mean_coefficients[-1] = -1 # set to -1 so that generated ratings have a standard deviation ~= 1 \n",
    "error_coefficients = np.random.normal(loc=0, scale=0.1, size=(num_sets, len(type_df), len(mean_coefficients)))\n",
    "type_specific_coefficients = mean_coefficients + error_coefficients\n",
    "\n",
    "# create a coefficient df\n",
    "# alpha = theta.(len(normalized_covariates))\n",
    "coeff_df = pd.DataFrame()\n",
    "coeff_df['type_idxs'] = np.arange(type_specific_coefficients.shape[1])\n",
    "for j in range(num_sets):\n",
    "    for i in range(1, type_specific_coefficients.shape[2]):\n",
    "        coeff_df['theta.{}_{}'.format(i, j)] = type_specific_coefficients[j, :, i]\n",
    "        \n",
    "# save mu = mean coefficients\n",
    "for i in range(1, type_specific_coefficients.shape[2]):\n",
    "    coeff_df['mean_theta.{}'.format(i)] = mean_coefficients[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11c0bb0c-15ad-4fe4-938b-874bcb440a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as mentioned earlier, the intercept (theta_k[0]) is set such that the generated ratings are mean 0 \n",
    "# thus, theta_k[0] = logit(E_t[T_{ikt}]) - theta_k[1:] X_i[1:]\n",
    "\n",
    "# in order to generate the intercepts we need\n",
    "# (i) synthetic values for theta_k[1:]\n",
    "# (ii) an estimate of E_t[T_{ikt}]\n",
    "\n",
    "# we have already generated (i) above\n",
    "# we now estimate E_t[T_{ikt}], or the empirical reporting frequencies, across the full dataset (train + test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30326fd1-7983-4af9-bc71-a82c6f81fa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get empirical E_t[T_{ikt}] or P(T) estimates from full dataset\n",
    "# E_t[T_{ikt}] is the mean value of T across time (t) for each node, type pair (i,k)\n",
    "empirical_pt = data_df.groupby(['type_idxs', 'node_idxs'])['finegrained_reported'].mean()\n",
    "empirical_pt_df = empirical_pt.reset_index()\n",
    "empirical_pt_df = empirical_pt_df.rename(columns={'finegrained_reported': 'P(T)'})\n",
    "# map to full dataset\n",
    "data_df = pd.merge(data_df, empirical_pt_df, on=['type_idxs', 'node_idxs'])\n",
    "\n",
    "# get number of data points for each node/type pair\n",
    "num_entries = data_df.groupby(['type_idxs', 'node_idxs']).size().reset_index()\n",
    "num_entries = num_entries.rename(columns={0: 'num_rows'})\n",
    "data_df = pd.merge(data_df, num_entries, on=['type_idxs', 'node_idxs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39c4fb0c-a433-4860-a5e9-7ef46c6916dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make semisynthetic data well specified, we do not want any node/type pairs where P(T) = 0 or P(T) = 1\n",
    "# thus we randomly bitflip one datapoint for these sets\n",
    "# first we define helper functions\n",
    "\n",
    "# Group by 'node' and 'type', and apply a lambda to set one random 'T' to 1 per group\n",
    "def set_random_one(group):\n",
    "    # Choosing a random index from the group's indices\n",
    "    random_index = np.random.choice(group.index)\n",
    "    # Setting 'T' to 1 at the chosen index\n",
    "    group.at[random_index, 'bitflip_reported'] = 1\n",
    "    return group\n",
    "\n",
    "# Group by 'node' and 'type', and apply a lambda to set one random 'T' to 0 per group\n",
    "def set_random_zero(group):\n",
    "    # Choosing a random index from the group's indices\n",
    "    random_index = np.random.choice(group.index)\n",
    "    # Setting 'T' to 1 at the chosen index\n",
    "    group.at[random_index, 'bitflip_reported'] = 0\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef326579-0938-4bdc-bd5c-7b2a71edc90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bitflip node/type pairs with P(T) = 0\n",
    "pt0_df = data_df[data_df['P(T)'] == 0].copy()\n",
    "pt0_df['bitflip_reported'] = pt0_df['finegrained_reported']\n",
    "\n",
    "# Apply the function across the DataFrame grouped by 'node' and 'type'\n",
    "pt0_df = pt0_df.groupby(['node_idxs', 'type_idxs']).apply(set_random_one).reset_index(drop=True)\n",
    "pt0_df['bitflip_P(T)'] = 1 / pt0_df['num_rows']\n",
    "\n",
    "# bitflip node/type pairs with P(T) = 1\n",
    "pt1_df = data_df[data_df['P(T)'] == 1].copy()\n",
    "pt1_df['bitflip_reported'] = pt1_df['finegrained_reported']\n",
    "\n",
    "# Apply the function across the DataFrame grouped by 'node' and 'type'\n",
    "pt1_df = pt1_df.groupby(['node_idxs', 'type_idxs']).apply(set_random_zero).reset_index(drop=True)\n",
    "pt1_df['bitflip_P(T)'] = (pt1_df['num_rows'] - 1) / pt1_df['num_rows']\n",
    "\n",
    "# add in bitflipped information\n",
    "no_bitflip = data_df[(data_df['P(T)'] != 0) & (data_df['P(T)'] != 1)].copy()\n",
    "no_bitflip['bitflip_reported'] = no_bitflip['finegrained_reported']\n",
    "no_bitflip['bitflip_P(T)'] = no_bitflip['P(T)']\n",
    "data_df = pd.concat([no_bitflip, pt0_df, pt1_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae9bf31e-ff81-4681-812d-ac5f3b4d297b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(coeff_df[coeff_df.isna().any(axis=1)]) == 0)\n",
    "assert(len(data_df[data_df.isna().any(axis=1)]) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e9b0ccb-31e3-4df1-86a7-3cc956e78230",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_df.to_csv('/share/garg/311_data/sb2377/clean_codebase/semisynthetic/semisynthetic_coeffs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3dbd3f90-8837-4b63-9ef4-85bfe4714629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/pierson/tmp_directory_location_please_read_readme/sb2377_tmp/ipykernel_972253/2408036819.py:1: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed-integer,key->block2_values] [items->Index(['typeagency', 'finegrained_id'], dtype='object')]\n",
      "\n",
      "  data_df.to_hdf('/share/garg/311_data/sb2377/clean_codebase/semisynthetic/semisynthetic_full.h5', key='df', mode='w')\n"
     ]
    }
   ],
   "source": [
    "data_df.to_hdf('/share/garg/311_data/sb2377/clean_codebase/semisynthetic/semisynthetic_full.h5', key='df', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165ba47e-e96e-417b-8502-9d18390eaf92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_gnn)",
   "language": "python",
   "name": "conda_gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

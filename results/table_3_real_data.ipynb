{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out correlation and rmse results for predicted P(T) and predicted ratings for real data for full model, reports only model, and ratings only model\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "\n",
    "# Set the font used for math expressions to LaTeX\n",
    "plt.rcParams[\"mathtext.fontset\"] = \"cm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file paths\n",
    "base_file = '/share/garg/311_data/sb2377/clean_codebase/three_year_base.csv'\n",
    "type_rating_observed_base_file = '/share/garg/311_data/sb2377/clean_codebase/three_year_type_rating_observed_base.csv'\n",
    "results_dir = '/share/garg/311_data/sb2377/results'\n",
    "\n",
    "# user specified arguments\n",
    "types = {'Street': 'StreetConditionDOT',\n",
    "         'Park': 'MaintenanceorFacilityDPR',\n",
    "         'Rodent': 'RodentDOHMH',\n",
    "         'Food': 'FoodDOHMH',\n",
    "         'DCWP': 'ConsumerComplaintDCWP'}\n",
    "models = {'Full model': {'job_ids':[3000] + [i * 3 + 3005 for i in range(12)]},\n",
    "          'Ratings-only model': {'job_ids':[3002] + [i * 3 + 3007 for i in range(12)]},\n",
    "          'Reports-only model': {'job_ids':[3001] + [i * 3 + 3006 for i in range(12)]}}\n",
    "epoch = '59'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load files\n",
    "base_df = pd.read_csv(base_file)\n",
    "type_rating_observed_base_df = pd.read_csv(type_rating_observed_base_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get type indices\n",
    "# for df with all types\n",
    "type_df = base_df[['typeagency', 'type_idxs']].drop_duplicates()\n",
    "indices = {}\n",
    "for type_name, type_id in types.items():\n",
    "    idx = type_df[type_df['typeagency'] == type_id]['type_idxs'].iloc[0]\n",
    "    indices[type_name] = idx\n",
    "\n",
    "# for df with only types with observed ratings\n",
    "type_df = type_rating_observed_base_df[['typeagency', 'type_idxs']].drop_duplicates()\n",
    "type_rating_observed_indices = {}\n",
    "for type_name, type_id in types.items():\n",
    "    idx = type_df[type_df['typeagency'] == type_id]['type_idxs'].iloc[0]\n",
    "    type_rating_observed_indices[type_name] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model: checkpoint files done = 13\n",
      "Full model: results files done = 13\n",
      "Ratings-only model: checkpoint files done = 13\n",
      "Ratings-only model: results files done = 13\n",
      "Reports-only model: checkpoint files done = 13\n",
      "Reports-only model: results files done = 13\n"
     ]
    }
   ],
   "source": [
    "# get predicted ratings for all jobs for types with observed ratings\n",
    "checkpoint_file = '{}/job{}/model-epoch={}.ckpt'\n",
    "results_file = '{}/job{}/epoch={}_test.pkl'\n",
    "checkpoint_counters = {}\n",
    "results_counters = {}\n",
    "for m in models:\n",
    "    checkpoint_counters[m] = 0\n",
    "    results_counters[m] = 0\n",
    "type_rating_observed_dfs = {}\n",
    "for m in models:\n",
    "    type_rating_observed_dfs[m] = []\n",
    "\n",
    "for m in models:\n",
    "    for i, job_idx in enumerate(models[m]['job_ids']):\n",
    "        if os.path.exists(checkpoint_file.format(results_dir, job_idx, epoch)):\n",
    "            checkpoint_counters[m] += 1\n",
    "        if os.path.exists(results_file.format(results_dir, job_idx, epoch)):\n",
    "            results_counters[m] += 1\n",
    "            with open(results_file.format(results_dir, job_idx, epoch), 'rb') as file:\n",
    "                pred_rating, true_rating, mask, node_embedding, type_embedding, node_idxs, type_idxs, demographics, pred_pt, true_t = pickle.load(file)\n",
    "\n",
    "            df = pd.DataFrame()\n",
    "            df['pred_rating'] = pred_rating\n",
    "            df['true_rating'] = true_rating\n",
    "            df['node_idxs'] = node_idxs\n",
    "            df['type_idxs'] = type_idxs\n",
    "            df['pred_pt'] = pred_pt\n",
    "            df['true_t'] = true_t\n",
    "            df['mask'] = mask\n",
    "\n",
    "            type_rating_observed_dfs[m].append(df)\n",
    "\n",
    "for m in models:\n",
    "    print('{}: checkpoint files done = {}'.format(m, checkpoint_counters[m]))\n",
    "    print('{}: results files done = {}'.format(m, results_counters[m]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model: checkpoint files done = 13\n",
      "Full model: results files done = 13\n",
      "Reports-only model: checkpoint files done = 13\n",
      "Reports-only model: results files done = 13\n"
     ]
    }
   ],
   "source": [
    "# get predicted P(T) and ratings for all jobs for types with unobserved ratings\n",
    "checkpoint_file = '{}/job{}/model-epoch={}.ckpt'\n",
    "results_file = '{}/job{}/epoch={}_test_unobserved.pkl'\n",
    "checkpoint_counters = {}\n",
    "results_counters = {}\n",
    "type_rating_unobserved_models = ['Full model', 'Reports-only model']\n",
    "for m in type_rating_unobserved_models:\n",
    "    checkpoint_counters[m] = 0\n",
    "    results_counters[m] = 0\n",
    "type_rating_unobserved_dfs = {}\n",
    "for m in type_rating_unobserved_models:\n",
    "    type_rating_unobserved_dfs[m] = []\n",
    "\n",
    "for m in type_rating_unobserved_models:\n",
    "    for i, job_idx in enumerate(models[m]['job_ids']):\n",
    "        if os.path.exists(checkpoint_file.format(results_dir, job_idx, epoch)):\n",
    "            checkpoint_counters[m] += 1\n",
    "        if os.path.exists(results_file.format(results_dir, job_idx, epoch)):\n",
    "            results_counters[m] += 1\n",
    "            with open(results_file.format(results_dir, job_idx, epoch), 'rb') as file:\n",
    "                pred_rating, true_rating, mask, node_embedding, type_embedding, node_idxs, type_idxs, demographics, pred_pt, true_t = pickle.load(file)\n",
    "\n",
    "            df = pd.DataFrame()\n",
    "            df['pred_rating'] = pred_rating\n",
    "            df['true_rating'] = true_rating\n",
    "            df['node_idxs'] = node_idxs\n",
    "            df['type_idxs'] = type_idxs\n",
    "            df['pred_pt'] = pred_pt\n",
    "            df['true_t'] = true_t\n",
    "            df['mask'] = mask\n",
    "\n",
    "            type_rating_unobserved_dfs[m].append(df)\n",
    "\n",
    "for m in type_rating_unobserved_models:\n",
    "    print('{}: checkpoint files done = {}'.format(m, checkpoint_counters[m]))\n",
    "    print('{}: results files done = {}'.format(m, results_counters[m]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine predicted P(T) and ratings for types with observed ratings and types with unobserved ratings\n",
    "dfs = {}\n",
    "for m in type_rating_unobserved_models:\n",
    "    dfs[m] = []\n",
    "    for i in range(len(type_rating_observed_dfs[m])):\n",
    "        full_df = pd.concat([type_rating_observed_dfs[m][i], type_rating_unobserved_dfs[m][i]])\n",
    "        dfs[m].append(full_df)\n",
    "dfs['Ratings-only model'] = type_rating_observed_dfs['Ratings-only model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Full model, Corr: 0.2371 \\pm 0.0110\n",
      "Model: Full model, RMSE: 0.1104 \\pm 0.0025\n",
      "Model: Reports-only model, Corr: 0.5406 \\pm 0.0063\n",
      "Model: Reports-only model, RMSE: 0.0566 \\pm 0.0012\n"
     ]
    }
   ],
   "source": [
    "# print out correlation and rmse results for predicted P(T)\n",
    "p_values_pt = {}\n",
    "for m in type_rating_unobserved_models:\n",
    "    df_set = dfs[m]\n",
    "    p_values_pt[m] = []\n",
    "    corrs = []\n",
    "    rmses = []\n",
    "    for idx in range(len(dfs[m][0]['type_idxs'].unique())):\n",
    "        type_corrs = []\n",
    "        type_rmses = []\n",
    "        for df in df_set:\n",
    "            df_type = df[df['type_idxs'] == idx]\n",
    "            node_df = df_type.groupby(['node_idxs', 'type_idxs']).mean().reset_index()\n",
    "\n",
    "            # calculate correlation\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                corr = pearsonr(node_df['pred_pt'], node_df['true_t'])\n",
    "            type_corrs.append(corr[0])\n",
    "            p_values_pt[m].append(corr[1])\n",
    "\n",
    "            # calculate rmse\n",
    "            rmse = np.sqrt(mean_squared_error(node_df['pred_pt'], node_df['true_t']))\n",
    "            type_rmses.append(rmse)\n",
    "\n",
    "        corrs.append(type_corrs)\n",
    "        rmses.append(type_rmses)\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        # calculate mean and 95% confidence interval over correlations\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        corrs = np.array(corrs)\n",
    "        filtered_corrs = corrs[~np.isnan(corrs).any(axis=1)]\n",
    "        mean_for_each_job = filtered_corrs.mean(axis=0)\n",
    "        mean_overall = filtered_corrs.mean()\n",
    "        se_across_jobs = np.std(mean_for_each_job) / np.sqrt(len(mean_for_each_job) - 1)\n",
    "        print('Model: {}, Corr: {:.4f} \\pm {:.4f}'.format(m, mean_overall, 1.96 * se_across_jobs))\n",
    "\n",
    "        # calculate mean and 95% confidence interval over rmses\n",
    "        rmses = np.array(rmses)\n",
    "        filtered_rmses = rmses[~np.isnan(rmses).any(axis=1)]\n",
    "        mean_for_each_job = filtered_rmses.mean(axis=0)\n",
    "        mean_overall = filtered_rmses.mean()\n",
    "        se_across_jobs = np.std(mean_for_each_job) / np.sqrt(len(mean_for_each_job) - 1)\n",
    "        print('Model: {}, RMSE: {:.4f} \\pm {:.4f}'.format(m, mean_overall, 1.96 * se_across_jobs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model 0.8273381294964028\n",
      "Reports-only model 0.9640287769784173\n"
     ]
    }
   ],
   "source": [
    "# print proportion of p-values < threshold for each model\n",
    "threshold = {'Full model': 0.01, 'Reports-only model': 0.01}\n",
    "for m in ['Full model', 'Reports-only model']:\n",
    "    count = 0\n",
    "    for v in p_values_pt[m]:\n",
    "        if v < threshold[m]:\n",
    "            count += 1\n",
    "    print(m, count / len(p_values_pt[m]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model 0.8516878804648589\n",
      "Reports-only model 0.9723298284449363\n"
     ]
    }
   ],
   "source": [
    "# print proportion of p-values < threshold for each model\n",
    "threshold = {'Full model': 0.1, 'Reports-only model': 0.1}\n",
    "for m in ['Full model', 'Reports-only model']:\n",
    "    count = 0\n",
    "    for v in p_values_pt[m]:\n",
    "        if v < threshold[m]:\n",
    "            count += 1\n",
    "    print(m, count / len(p_values_pt[m]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Full model, Corr: 0.5303 \\pm 0.0194\n",
      "Model: Full model, RMSE: 0.5833 \\pm 0.0123\n",
      "Model: Ratings-only model, Corr: 0.5223 \\pm 0.0185\n",
      "Model: Ratings-only model, RMSE: 0.5852 \\pm 0.0116\n",
      "Model: Reports-only model, Corr: 0.0993 \\pm 0.0143\n",
      "Model: Reports-only model, RMSE: nan \\pm nan\n"
     ]
    }
   ],
   "source": [
    "# print out correlation and rmse results for predicted ratings\n",
    "p_values_r = {}\n",
    "for m in models:\n",
    "    df_set = type_rating_observed_dfs[m]\n",
    "    p_values_r[m] = []\n",
    "    corrs = []\n",
    "    rmses = []\n",
    "    for t in types:\n",
    "        type_corrs = []\n",
    "        type_rmses = []\n",
    "        idx = indices[t]\n",
    "        type_rating_observed_idx = type_rating_observed_indices[t]\n",
    "        for df in df_set:\n",
    "            df_type = df[df['type_idxs'] == idx]\n",
    "            if m == 'Ratings-only model':\n",
    "                df_type = df[df['type_idxs'] == type_rating_observed_idx]\n",
    "            else:\n",
    "                df_type = df[df['type_idxs'] == idx]\n",
    "            node_df = df_type.groupby(['node_idxs', 'type_idxs']).mean().reset_index()\n",
    "\n",
    "            with warnings.catch_warnings():\n",
    "                if m == 'Reports-only model':\n",
    "                    # for reports-only model, we use -P(T) as a proxy for r\n",
    "                    corr = pearsonr(-1 * node_df['pred_pt'], node_df['true_rating'])\n",
    "                else:\n",
    "                    corr = pearsonr(node_df['pred_rating'], node_df['true_rating'])\n",
    "            type_corrs.append(corr[0])\n",
    "            p_values_r[m].append(corr[1])\n",
    "\n",
    "            # calculate rmse\n",
    "            if m == 'Reports-only model':\n",
    "                rmse = np.nan\n",
    "            else:\n",
    "                rmse = np.sqrt(mean_squared_error(node_df['pred_rating'], node_df['true_rating']))\n",
    "            type_rmses.append(rmse)\n",
    "\n",
    "        corrs.append(type_corrs)\n",
    "        rmses.append(type_rmses)\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        # calculate mean and 95% confidence interval over correlations\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        corrs = np.array(corrs)\n",
    "        filtered_corrs = corrs[~np.isnan(corrs).any(axis=1)]\n",
    "        mean_for_each_job = filtered_corrs.mean(axis=0)\n",
    "        mean_overall = filtered_corrs.mean()\n",
    "        se_across_jobs = np.std(mean_for_each_job) / np.sqrt(len(mean_for_each_job) - 1)\n",
    "        print('Model: {}, Corr: {:.4f} \\pm {:.4f}'.format(m, mean_overall, 1.96 * se_across_jobs))\n",
    "\n",
    "        # calculate mean and 95% confidence interval over rmses\n",
    "        rmses = np.array(rmses)\n",
    "        filtered_rmses = rmses[~np.isnan(rmses).any(axis=1)]\n",
    "        mean_for_each_job = filtered_rmses.mean(axis=0)\n",
    "        mean_overall = filtered_rmses.mean()\n",
    "        se_across_jobs = np.std(mean_for_each_job) / np.sqrt(len(mean_for_each_job) - 1)\n",
    "        print('Model: {}, RMSE: {:.4f} \\pm {:.4f}'.format(m, mean_overall, 1.96 * se_across_jobs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model 1.0\n",
      "Ratings-only model 1.0\n",
      "Reports-only model 0.4461538461538462\n"
     ]
    }
   ],
   "source": [
    "# print proportion of p-values < threshold for each model\n",
    "threshold = {'Full model': 0.01, 'Ratings-only model': 0.01, 'Reports-only model': 0.01}\n",
    "for m in models:\n",
    "    count = 0\n",
    "    for v in p_values_r[m]:\n",
    "        if v < threshold[m]:\n",
    "            count += 1\n",
    "    print(m, count / len(p_values_r[m]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model 1.0\n",
      "Ratings-only model 1.0\n",
      "Reports-only model 0.5384615384615384\n"
     ]
    }
   ],
   "source": [
    "# print proportion of p-values < threshold for each model\n",
    "threshold = {'Full model': 0.05, 'Ratings-only model': 0.05, 'Reports-only model': 0.05}\n",
    "for m in models:\n",
    "    count = 0\n",
    "    for v in p_values_r[m]:\n",
    "        if v < threshold[m]:\n",
    "            count += 1\n",
    "    print(m, count / len(p_values_r[m]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model 1.0\n",
      "Ratings-only model 1.0\n",
      "Reports-only model 0.6\n"
     ]
    }
   ],
   "source": [
    "# print proportion of p-values < threshold for each model\n",
    "threshold = {'Full model': 0.1, 'Ratings-only model': 0.1, 'Reports-only model': 0.1}\n",
    "for m in models:\n",
    "    count = 0\n",
    "    for v in p_values_r[m]:\n",
    "        if v < threshold[m]:\n",
    "            count += 1\n",
    "    print(m, count / len(p_values_r[m]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_gnn)",
   "language": "python",
   "name": "conda_gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
